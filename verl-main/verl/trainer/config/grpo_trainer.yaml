data:
  tokenizer: null
  train_files: /home/wxf/lzq/twq/verl-main/examples/datasets/svg/train.parquet
  val_files: /home/wxf/lzq/twq/verl-main/examples/datasets/svg/train.parquet
  prompt_key: prompt
  max_prompt_length: 1024
  max_response_length: 4096
  # 训练批量大小，每个step的batch_size
  train_batch_size: 4
  val_batch_size: 4
  return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs
  return_raw_chat: False
  shuffle: True
  system_prompt: 'You are Qwen, You are a helpful svg designer.Given a text prompt describing an image, your task is to generate Scalable Vector Graphics (SVG) code that renders it as an image as closely as possible, you can not say anything else but the code.
The descriptions have the following properties:
The svg illustration size is 384*384
The descriptions are of common, generic subjects. No brand name or trademark or personal name occurs in any description. No people, even in generic form, occur in any description.
The subjects described span about a dozen categories. Three of these categories, landscapes, abstract, and fashion, are present in each of the training, public test, and private test sets. The remaining categories in the public and private sets are unique to each set. More than half of the descriptions come from the three shared categories.
No description has more than 200 characters. The average length is around 50 characters.'
  # prompt超过最大长度时采取的截断方式  
  truncation: left   # error/left/right

actor_rollout_ref:
  hybrid_engine: True
  model:
    path: /home/wxf/lzq/data/model/qwen/qwen2.5-coder-7b
    # 动态导入外部库
    external_lib: null
    # 覆盖配置
    override_config: { }
    # 启用梯度检查点
    enable_gradient_checkpointing: False
    # 使用移除填充
    use_remove_padding: True
    use_lora: False
    lora_config:
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
  actor:
    # 加速策略
    strategy: fsdp  # This is for backward-compatibility
    # 小批量大小，梯度更新的频率
    ppo_mini_batch_size: 16
    # 微批量大小，每个gpu分配的batch_size
    ppo_micro_batch_size_per_gpu: 16
    # 使用动态批量大小
    use_dynamic_bsz: False
    # 最大token长度，当use_dynamic_bsz为True时使用
    ppo_max_token_len_per_gpu: 8192 # n * ${data.max_prompt_length} + ${data.max_response_length}
    # 梯度裁剪
    grad_clip: 1.0
    # 裁剪比率
    clip_ratio: 0.2
    # 熵系数
    entropy_coeff: 0.001
    # 使用kl损失
    use_kl_loss: True # True for GRPO
    # kl损失系数
    kl_loss_coef: 0.001 # for grpo
    # kl损失类型，低方差近似
    kl_loss_type: low_var_kl # for grpo
    # ppo轮数，起提示作用
    ppo_epochs: 1
    # 是否打乱数据
    shuffle: False
    # 序列并行大小
    ulysses_sequence_parallel_size: 2 # sp size
    # 优化器
    optim:
      lr: 1e-5
      # 学习率预热步数比率
      lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime
      # 最小学习率比率
      min_lr_ratio: null   # only useful for warmup with cosine
      # 预热风格
      warmup_style: constant  # select from constant/cosine
      # 每个epoch最大的训练步数
      total_training_steps: null  # must be override by program
    # fsdp配置
    fsdp_config:
      model_dtype: bfloat16
      # 包装策略
      wrap_policy:
        # transformer_layer_cls_to_wrap: None
        min_num_params: 0
      # 参数卸载
      param_offload: True
      # 梯度卸载
      grad_offload: True
      # 优化器卸载
      optimizer_offload: True
      # fsdp大小
      fsdp_size: -1
  # 引用
  ref:
    fsdp_config:
      param_offload: True
      wrap_policy:
        # transformer_layer_cls_to_wrap: None
        min_num_params: 0
    micro_batch_size_per_gpu: 128
    # 微批量大小
    log_prob_micro_batch_size_per_gpu: 128
    # 使用动态批量大小
    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}
    # 最大token长度
    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}
    # 序列并行大小
    ulysses_sequence_parallel_size: ${actor_rollout_ref.actor.ulysses_sequence_parallel_size} # sp size
  rollout:
    # 推理框架
    name: vllm
    # 温度
    temperature: 1.0
    # top_k
    top_k: -1 # 0 for hf rollout, -1 for vllm rollout
    # top_p
    top_p: 0.8
    # 提示长度
    prompt_length: ${data.max_prompt_length}  # not use for opensource
    # 响应长度
    response_length: ${data.max_response_length}
    # for vllm rollout
    dtype: bfloat16 # should align with FSDP
    # gpu利用率
    gpu_memory_utilization: 0.7
    # 忽略eos
    ignore_eos: False
    # 强制急切
    enforce_eager: True
    # 释放vllm引擎的缓存
    free_cache_engine: True
    # 加载格式
    load_format: dummy_dtensor
    # 张量模型并行大小
    tensor_model_parallel_size: 2
    # 最大批量token数
    max_num_batched_tokens: 8192
    # 最大序列数
    max_num_seqs: 1024
    # 微批量大小
    log_prob_micro_batch_size_per_gpu: 128
    # 使用动态批量大小
    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}
    # 最大token长度
    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}
    # 禁用日志统计
    disable_log_stats: True
    # 启用分块预填
    enable_chunked_prefill: True # could get higher throughput
    # for hf rollout
    do_sample: True
    # number of responses (i.e. num sample times)
    n: 8 # > 1 for grpo

reward_model:
  # 使用reward model
  enable: False
  strategy: fsdp
  model:
    input_tokenizer: ${actor_rollout_ref.model.path}  # set this to null if the chat template is identical
    path: /home/wxf/lzq/data/model/paligemma2-10b-mix-448/models--google--paligemma2-10b-mix-448/snapshots/b26d16fb4251090ba4a4aa5af9fca1f8248ed5b6
    external_lib: ${actor_rollout_ref.model.external_lib}
    fsdp_config:
      min_num_params: 0
      param_offload: True
      # fsdp大小
      fsdp_size: -1
  micro_batch_size_per_gpu: 8 # set a number
  ulysses_sequence_parallel_size: 1 # sp size
  # the max length of prompt
  max_length: 1024
  use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}
  forward_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}
  reward_manager: naive

algorithm:
  # 折扣因子
  gamma: 1.0
  # 衰减因子
  lam: 1.0
  # 优势估计器
  adv_estimator: grpo
  # kl惩罚
  kl_penalty: kl  # how to estimate kl divergence
  kl_ctrl:
    type: fixed
    kl_coef: 0.001

trainer:
  # total_training_steps = len(self.train_dataloader) * self.config.trainer.total_epochs
  total_epochs: 15
  # 定义了总的训练步数会覆盖total_epochs
  total_training_steps: null
  project_name: verl_grpo_SVG
  experiment_name: qwen25_7
  logger: [ 'console', 'tensorboard']
  tensorboard_dir: outputs/tensorboard/${trainer.project_name}/${trainer.experiment_name}
  # 节点数
  nnodes: 1
  # 每个节点gpu数
  n_gpus_per_node: 4
  # 保存频率
  save_freq: 250
  # auto: find the last ckpt to resume. If can't find, start from scratch
  resume_mode: auto # or auto or resume_path if 
  resume_from_path: False
  # 测试频率
  test_freq: -1
  val_before_train: False
  # 批评者预热步数
  critic_warmup: 0
  # 默认hdfs目录
  default_hdfs_dir: null
  # 默认本地目录
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  load_dataloader: False